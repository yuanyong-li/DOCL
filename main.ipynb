{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from textwrap import dedent\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from utils.pydantic import Class, RAG_Analyse_Class\n",
    "from utils.tools import (\n",
    "    get_vectordb_length, clear, md, green,\n",
    "    grey, similarity_search, build_reference_examples_chain,\n",
    "    docl_search, extract_json\n",
    ")\n",
    "from utils.prompt import docl_prompt_template, zs_prompt_template, fs_prompt_template, rag_prompt_template\n",
    "from utils.langsmith import activate_langsmith, remove_langsmith\n",
    "from utils.embedding import BGEM3Embeddings, BERTEmbeddings\n",
    "from utils import api_config\n",
    "from config.llm_setting import llm_config\n",
    "\n",
    "# activate_langsmith()\n",
    "# remove_langsmith()·\n",
    "\n",
    "\n",
    "dataset = 'LatentHatred' #[Latent_Hatred]\n",
    "method = 'cocl' #[zs, fs, rag, cocl]\n",
    "llm_name = \"llama3-70b-instruct\" #[gpt-3.5-turbo, llama3-70b-instruct]\n",
    "train_data_path = f'./data/{dataset}/IH_train.csv'\n",
    "test_data_path = f'./data/{dataset}/IH_test.csv'\n",
    "model_path = './model'\n",
    "persist_directory = './Chroma'\n",
    "emb_device = 'cuda:0'\n",
    "llm_device = 'cuda:4'\n",
    "encode_batch_size=256\n",
    "# 'bert' or 'bgem3' or 'ftodel'\n",
    "embedding_model = 'ftmodel'\n",
    "output_path = f'./results/{dataset}_{method}_{llm_name}_{embedding_model}.csv'\n",
    "finetune_data_path = f'./finetune/{method}_ft_data.json'\n",
    "finetuned_model_path = './finetune/cocl_finetuned_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_name.startswith('gpt'):\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.95,\n",
    "        model=llm_name,\n",
    "        openai_api_key=api_config.openai_key,\n",
    "        openai_api_base=api_config.openai_base\n",
    "    )\n",
    "\n",
    "elif llm_name == \"llama3-70b-instruct\":\n",
    "    llm = ChatOpenAI(\n",
    "        temperature=0.95,\n",
    "        model=llm_name,\n",
    "        openai_api_key=api_config.openai_key,\n",
    "        openai_api_base=api_config.openai_base\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method != 'zs' and method != 'fs':\n",
    "    if embedding_model == 'bgem3':\n",
    "        model = AutoModel.from_pretrained(\"BAAI/bge-m3\", cache_dir=model_path, local_files_only=True).to(emb_device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\", cache_dir=model_path, local_files_only=True)\n",
    "        embeddings = BGEM3Embeddings(model, tokenizer, encode_batch_size)\n",
    "        \n",
    "    elif embedding_model == 'bert':\n",
    "        model = AutoModel.from_pretrained(\"./model/bert-base-uncased\", cache_dir=model_path, local_files_only=True).to(emb_device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"./model/bert-base-uncased\", cache_dir=model_path, local_files_only=True)\n",
    "        embeddings = BERTEmbeddings(model, tokenizer, encode_batch_size)\n",
    "        \n",
    "    elif embedding_model == 'ftmodel':\n",
    "        model = AutoModel.from_pretrained(finetuned_model_path, cache_dir=model_path).to(emb_device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path, cache_dir=model_path)\n",
    "        embeddings = BGEM3Embeddings(model, tokenizer, encode_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chain(method):\n",
    "    if method == 'zs':\n",
    "        chain = (\n",
    "            RunnableLambda(lambda x: x['input_example'])\n",
    "            | zs_prompt_template\n",
    "            | (llm.with_structured_output(Class) if llm_config[llm_name]['structured_output'] else llm)\n",
    "        )\n",
    "        print(\"zs_chain built\")\n",
    "\n",
    "    elif method == 'fs':\n",
    "        chain = (\n",
    "            RunnableLambda(lambda x: x['input_example'])\n",
    "            | fs_prompt_template\n",
    "            | (llm.with_structured_output(Class) if llm_config[llm_name]['structured_output'] else llm)\n",
    "        )\n",
    "        print(\"fs_chain built\")\n",
    "\n",
    "    elif method == \"rag\" or method == \"cocl\" or method == 'docl':\n",
    "        df = pd.read_csv(train_data_path)\n",
    "        documents = [\n",
    "            Document(page_content=row['post'], metadata={'class': row['class']})\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "        # documents = documents[:1000]\n",
    "        categories = set([doc.metadata['class'] for doc in documents])\n",
    "        shutil.rmtree(persist_directory, ignore_errors=True)\n",
    "        vectordb = Chroma.from_documents(documents, embeddings)\n",
    "        print(get_vectordb_length(vectordb))\n",
    "        \n",
    "        if method == \"rag\" or \"cocl\":\n",
    "            chain = (\n",
    "                RunnableLambda(lambda x: {\"sim_docs\": similarity_search(vectordb, x[\"input_example\"], x.get('k', 6)), \"input_example\": x[\"input_example\"]})  # 相似度搜索\n",
    "                | RunnableLambda(lambda x: {\"reference_examples\": build_reference_examples_chain(x[\"sim_docs\"]), \"input_example\": x[\"input_example\"]})  # 生成参考示例\n",
    "                # | RunnableLambda(lambda x: md(docl_prompt_template.format(**x)) or x)  # Print formatted prompt\n",
    "                # | RunnableLambda(lambda x: md(x['reference_examples'], f\"{green('Input_post')}: {x['input_example']}\") or x)  # Print the Input\n",
    "                | rag_prompt_template  # 提示模板\n",
    "                | (llm.with_structured_output(RAG_Analyse_Class) if llm_config[llm_name]['structured_output'] else llm)\n",
    "            )\n",
    "        if method == 'docl':\n",
    "            chain = (\n",
    "                RunnableLambda(lambda x: {\"sim_docs\": docl_search(vectordb, x[\"input_example\"], x.get('k', 6)), \"input_example\": x[\"input_example\"]})  # 相似度搜索\n",
    "                | RunnableLambda(lambda x: {\"reference_examples\": build_reference_examples_chain(x[\"sim_docs\"]), \"input_example\": x[\"input_example\"]})  # 生成参考示例\n",
    "                # | RunnableLambda(lambda x: md(docl_prompt_template.format(**x)) or x)  # Print formatted prompt\n",
    "                # | RunnableLambda(lambda x: md(x['reference_examples'], f\"{green('Input_post')}: {x['input_example']}\") or x)  # Print the Input\n",
    "                | rag_prompt_template  # 提示模板\n",
    "                | (llm.with_structured_output(RAG_Analyse_Class) if llm_config[llm_name]['structured_output'] else llm)\n",
    "            )\n",
    "    return chain\n",
    "\n",
    "chain = build_chain(method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_data_path, output_path, chain, num=None, k=None):\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    test_df['pred'] = None\n",
    "    if num is not None:\n",
    "        test_df = test_df[:num]\n",
    "\n",
    "    if dataset == 'LatentHatred':\n",
    "        def process_row(idx, post):\n",
    "            # 调用预测模型\n",
    "            try:\n",
    "                input_data = {\n",
    "                    \"input_example\": post\n",
    "                }\n",
    "                if k is not None:\n",
    "                    input_data[\"k\"] = k\n",
    "                result = chain.invoke(input_data)\n",
    "                if llm_config[llm_name]['structured_output']:\n",
    "                    predict_label = result.dict()['result']\n",
    "                else:\n",
    "                    content = result.dict()['content']\n",
    "                    content_json = extract_json(content)\n",
    "                    predict_label = json.loads(content_json)['result']\n",
    "                return idx, predict_label\n",
    "            except Exception as e:\n",
    "                # print(f\"Exception caught in chain: {content}\")\n",
    "                return idx, \"error\"  # Merge with original input if needed\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "            # 提交任务\n",
    "            futures = {executor.submit(process_row, idx, row['post']): idx for idx, row in test_df.iterrows()}\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=len(futures), mininterval=2.0):\n",
    "                idx, predict_label = future.result()\n",
    "                if idx % 100 == 0:\n",
    "                    test_df.to_csv(output_path, index=False)\n",
    "                test_df.at[idx, 'pred'] = predict_label\n",
    "\n",
    "        # test_df['pred'] = test_df['pred'].apply(lambda x: 'not_hate' if 'B' in x.upper() else 'hate')\n",
    "    return test_df\n",
    "\n",
    "def calculate_metrics(test_result):\n",
    "    label = [0 if 'not' in cls.lower() else 1 for cls in test_result['class'].tolist()]\n",
    "    pred = [0 if 'not' in cls.lower() else 1 for cls in test_result['pred'].tolist()]\n",
    "\n",
    "    precision = precision_score(label, pred)\n",
    "    recall = recall_score(label, pred)\n",
    "    f1 = f1_score(label, pred)\n",
    "    accuracy = accuracy_score(label, pred)\n",
    "    conf_matrix = confusion_matrix(label, pred)\n",
    "    \n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Confusion Matrix: \\n{conf_matrix}\")\n",
    "    \n",
    "    return precision, recall, f1, accuracy, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test(test_data_path, output_path, chain, num=100, k=20)\n",
    "p, r, f1, acc, cm = calculate_metrics(test_result[test_result.loc[:, 'pred'] != \"error\"])\n",
    "test_result.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_result = pd.read_csv(\"/home/liyuanyong2022/DOCL/results/LatentHatred_cocl_llama3-70b-instruct_ftmodel.csv\")\n",
    "temp_result = temp_result[temp_result.loc[:, 'pred'].notnull()]\n",
    "temp_result = temp_result[:1000]\n",
    "p, r, f1, acc, cm = calculate_metrics(temp_result[temp_result.loc[:, 'pred'] != \"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_result = pd.read_csv(\"/home/liyuanyong2022/DOCL/results/LatentHatred_zs_llama3-70b-instruct_bgem3.csv\")\n",
    "temp_result = temp_result[temp_result.loc[:, 'pred'].notnull()]\n",
    "temp_result = temp_result[:1000]\n",
    "p, r, f1, acc, cm = calculate_metrics(temp_result[temp_result.loc[:, 'pred'] != \"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[test_result.loc[:, 'pred'] != \"error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "set_verbose(True)\n",
    "set_debug(True)\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"input_example\": \"white americans are committing suicide in reco...\t\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Train Data(COCL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'cocl':\n",
    "    def build_data(x):\n",
    "        cla = 'hate' if x['class'] in ['implicit_hate', 'explicit_hate'] else x['class']\n",
    "        \n",
    "        reference_examples = similarity_search_chain(vectordb, x[\"input_example\"], k=50)\n",
    "        reference_examples = [\n",
    "            {'class': 'hate' if e['class'] in ['implicit_hate', 'explicit_hate'] else e['class'], 'post': e['post']}\n",
    "            for e in reference_examples\n",
    "        ]\n",
    "        query = x['input_example']\n",
    "        pos_list = []\n",
    "        neg_list = []\n",
    "        for example in reference_examples:\n",
    "            if example['class'] == cla:\n",
    "                pos_list.append(example['post'])\n",
    "            else:\n",
    "                neg_list.append(example['post'])\n",
    "        \n",
    "        if len(pos_list) == 0 or len(neg_list) == 0:\n",
    "            return None\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"pos\": pos_list,\n",
    "            \"neg\": neg_list\n",
    "        }\n",
    "    \n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    output_data = []\n",
    "    for idx, data in tqdm(test_data.iterrows(), total=len(test_data), mininterval=2.0):\n",
    "        result = build_data({\"input_example\": data['post'], \"class\": data['class']})\n",
    "        if result:\n",
    "            output_data.append(result)\n",
    "    with open(finetune_data_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_data_path)\n",
    "documents = [\n",
    "    Document(page_content=row['post'], metadata={'class': row['class']})\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "# documents = documents[:1000]\n",
    "categories = set([doc.metadata['class'] for doc in documents])\n",
    "shutil.rmtree(persist_directory, ignore_errors=True)\n",
    "vectordb = Chroma.from_documents(documents, embeddings)\n",
    "print(get_vectordb_length(vectordb))\n",
    "\n",
    "if method == \"rag\" or \"cocl\":\n",
    "    chain = (\n",
    "        RunnableLambda(lambda x: {\"sim_docs\": similarity_search(vectordb, x[\"input_example\"], x.get('k', 6)), \"input_example\": x[\"input_example\"]})  # 相似度搜索\n",
    "        | RunnableLambda(lambda x: {\"reference_examples\": build_reference_examples_chain(x[\"sim_docs\"]), \"input_example\": x[\"input_example\"]})  # 生成参考示例\n",
    "        # | RunnableLambda(lambda x: md(docl_prompt_template.format(**x)) or x)  # Print formatted prompt\n",
    "        # | RunnableLambda(lambda x: md(x['reference_examples'], f\"{green('Input_post')}: {x['input_example']}\") or x)  # Print the Input\n",
    "        | rag_prompt_template  # 提示模板\n",
    "        | llm.with_structured_output(RAG_Analyse_Class)  # Pass the prompt string to llm\n",
    "    )\n",
    "if method == 'docl':\n",
    "    chain = (\n",
    "        RunnableLambda(lambda x: {\"sim_docs\": docl_search(vectordb, x[\"input_example\"], x.get('k', 6)), \"input_example\": x[\"input_example\"]})  # 相似度搜索\n",
    "        | RunnableLambda(lambda x: {\"reference_examples\": build_reference_examples_chain(x[\"sim_docs\"]), \"input_example\": x[\"input_example\"]})  # 生成参考示例\n",
    "        # | RunnableLambda(lambda x: md(docl_prompt_template.format(**x)) or x)  # Print formatted prompt\n",
    "        # | RunnableLambda(lambda x: md(x['reference_examples'], f\"{green('Input_post')}: {x['input_example']}\") or x)  # Print the Input\n",
    "        | rag_prompt_template  # 提示模板\n",
    "        | llm.with_structured_output(RAG_Analyse_Class)  # Pass the prompt string to llm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docl_chain = (\n",
    "    RunnableLambda(lambda x: {\"sim_docs\": docl_search(vectordb, x[\"input_example\"], x.get('k', 6)), \"input_example\": x[\"input_example\"]})  # 相似度搜索\n",
    "    | RunnableLambda(lambda x: {\"reference_examples\": build_reference_examples_chain(x[\"sim_docs\"]), \"input_example\": x[\"input_example\"]})  # 生成参考示例\n",
    "    # | RunnableLambda(lambda x: md(docl_prompt_template.format(**x)) or x)  # Print formatted prompt\n",
    "    | RunnableLambda(lambda x: md(x['reference_examples'], f\"{green('Input_post')}: {x['input_example']}\") or x)  # Print the Input\n",
    "    | rag_prompt_template  # 提示模板\n",
    "    # | llm.with_structured_output(RAG_Analyse_Class)  # Pass the prompt string to llm\n",
    ")\n",
    "cocl_chain = (\n",
    "    RunnableLambda(lambda x: {\"sim_docs\": similarity_search(vectordb, x[\"input_example\"], x.get('k', 6)), \"input_example\": x[\"input_example\"]})  # 相似度搜索\n",
    "    | RunnableLambda(lambda x: {\"reference_examples\": build_reference_examples_chain(x[\"sim_docs\"]), \"input_example\": x[\"input_example\"]})  # 生成参考示例\n",
    "    # | RunnableLambda(lambda x: md(docl_prompt_template.format(**x)) or x)  # Print formatted prompt\n",
    "    | RunnableLambda(lambda x: md(x['reference_examples'], f\"{green('Input_post')}: {x['input_example']}\") or x)  # Print the Input\n",
    "    | rag_prompt_template  # 提示模板\n",
    "    # | llm.with_structured_output(RAG_Analyse_Class)  # Pass the prompt string to llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_result = pd.read_csv(\"/home/liyuanyong2022/DOCL/results/LatentHatred_docl_gpt-3.5-turbo_ftmodel.csv\")\n",
    "temp_result = temp_result[temp_result.loc[:, 'pred'].notnull()]\n",
    "\n",
    "for idx, data in temp_result.iterrows():\n",
    "    if data['class'] != data['pred']:\n",
    "        result = docl_chain.invoke({\n",
    "            \"input_example\": data['post'],\n",
    "            \"k\": 12\n",
    "        })\n",
    "        md(f\"{grey('label: ')} {data['class']}\")\n",
    "        result = cocl_chain.invoke({\n",
    "            \"input_example\": data['post'],\n",
    "            \"k\": 12\n",
    "        })\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DOCL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
